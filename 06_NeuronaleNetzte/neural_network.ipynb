{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052a030f",
   "metadata": {},
   "source": [
    "Einfaches Neuronales Netzwerk mit Backpropagation\n",
    "===============================================\n",
    "\n",
    "Dieses Programm implementiert ein einfaches neuronales Netzwerk, das die grundlegenden\n",
    "Konzepte des maschinellen Lernens demonstriert. Es verwendet die Backpropagation-Methode,\n",
    "um das XOR-Problem zu lösen.\n",
    "\n",
    "Architektur:\n",
    "-----------\n",
    "- Eingabeschicht: 2 Neuronen\n",
    "- Versteckte Schicht: 2 Neuronen\n",
    "- Ausgabeschicht: 1 Neuron\n",
    "\n",
    "Funktionsweise:\n",
    "-------------\n",
    "1. Initialisierung:\n",
    "   - Zufällige Gewichte und Bias-Terme werden erstellt\n",
    "   - Jede Verbindung zwischen Neuronen hat ein eigenes Gewicht\n",
    "\n",
    "2. Forward Propagation:\n",
    "   - Eingabedaten werden durch das Netzwerk geleitet\n",
    "   - Jede Schicht berechnet ihre Ausgabe mit der Sigmoid-Funktion\n",
    "   - Zwischenergebnisse werden für Backpropagation gespeichert\n",
    "\n",
    "3. Backpropagation:\n",
    "   - Fehler zwischen Vorhersage und tatsächlichem Wert wird berechnet\n",
    "   - Fehler wird rückwärts durch das Netzwerk propagiert\n",
    "   - Gewichte werden basierend auf dem Fehler angepasst\n",
    "\n",
    "4. Training:\n",
    "   - 10.000 Epochen werden durchgeführt\n",
    "   - In jeder Epoche: Forward Propagation → Backpropagation\n",
    "   - Verlust wird alle 1000 Epochen ausgegeben\n",
    "\n",
    "XOR-Problem:\n",
    "----------\n",
    "Das Programm demonstriert das Lernen am XOR-Problem:\n",
    "Eingabe    Ausgabe\n",
    "0 0    →   0\n",
    "0 1    →   1\n",
    "1 0    →   1\n",
    "1 1    →   0\n",
    "\n",
    "Dies ist ein klassisches Problem, das ein einfaches Perzeptron nicht lösen kann,\n",
    "aber mit einem mehrschichtigen Netzwerk lösbar ist.\n",
    "\n",
    "Programmausgabe:\n",
    "-------------\n",
    "1. Trainingsphase:\n",
    "   - \"Starte Training...\" wird angezeigt\n",
    "   - Alle 1000 Epochen wird der aktuelle Verlust (Loss) ausgegeben\n",
    "   - Format: \"Epoche X, Verlust: Y\"\n",
    "   - Der Verlust sollte mit der Zeit kleiner werden\n",
    "\n",
    "2. Testphase:\n",
    "   - \"Teste das trainierte Netzwerk:\" wird angezeigt\n",
    "   - Für jede XOR-Eingabe wird ausgegeben:\n",
    "     * Die Eingabewerte\n",
    "     * Die erwartete Ausgabe\n",
    "     * Die tatsächliche Vorhersage des Netzwerks (auf 4 Dezimalstellen gerundet)\n",
    "   - Format: \"Eingabe: [X Y], Erwartete Ausgabe: Z, Vorhersage: W\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34334c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Training...\n",
      "Epoche 0, Verlust: 0.2749328734522748\n",
      "Epoche 1000, Verlust: 0.24999788529354813\n",
      "Epoche 2000, Verlust: 0.2492537425228899\n",
      "Epoche 3000, Verlust: 0.24221663923085698\n",
      "Epoche 4000, Verlust: 0.12749086775844143\n",
      "Epoche 5000, Verlust: 0.026244307747929504\n",
      "Epoche 6000, Verlust: 0.011269125231825713\n",
      "Epoche 7000, Verlust: 0.006773445248895694\n",
      "Epoche 8000, Verlust: 0.004740746151418473\n",
      "Epoche 9000, Verlust: 0.003609519942131666\n",
      "\n",
      "Teste das trainierte Netzwerk:\n",
      "Eingabe: [0 0], Erwartete Ausgabe: 0, Vorhersage: 0.0596\n",
      "Eingabe: [0 1], Erwartete Ausgabe: 1, Vorhersage: 0.9486\n",
      "Eingabe: [1 0], Erwartete Ausgabe: 1, Vorhersage: 0.9487\n",
      "Eingabe: [1 1], Erwartete Ausgabe: 0, Vorhersage: 0.0527\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"\n",
    "    Einfaches neuronales Netzwerk mit Backpropagation.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialisiert das neuronale Netzwerk mit zufälligen Gewichten und Biases.\n",
    "        - weights1: Verbindungen zwischen Eingabe- und versteckter Schicht (2x2 Matrix)\n",
    "        - weights2: Verbindungen zwischen versteckter und Ausgabeschicht (2x1 Matrix)\n",
    "        - bias1: Bias-Terme für die versteckte Schicht (2 Vektoren)\n",
    "        - bias2: Bias-Term für die Ausgabeschicht (1 Vektor)\n",
    "        \"\"\"\n",
    "        self.weights1 = np.random.randn(2, 2)  # Gewichte Eingabe → versteckte Schicht\n",
    "        self.weights2 = np.random.randn(2, 1)  # Gewichte versteckte Schicht → Ausgabe\n",
    "        self.bias1 = np.random.randn(2)        # Bias für versteckte Schicht\n",
    "        self.bias2 = np.random.randn(1)        # Bias für Ausgabeschicht\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid-Aktivierungsfunktion.\n",
    "        Komprimiert die Eingabe auf einen Wert zwischen 0 und 1.\n",
    "        Formel: f(x) = 1 / (1 + e^(-x))\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        Ableitung der Sigmoid-Funktion.\n",
    "        Wird für die Backpropagation benötigt.\n",
    "        Formel: f'(x) = f(x) * (1 - f(x))\n",
    "        \"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Führt den Vorwärtsdurchlauf durch das Netzwerk durch.\n",
    "        \n",
    "        Parameter:\n",
    "        - X: Eingabedaten (Matrix)\n",
    "        \n",
    "        Prozess:\n",
    "        1. Berechnet die Ausgabe der versteckten Schicht\n",
    "        2. Berechnet die finale Ausgabe\n",
    "        3. Speichert Zwischenergebnisse für Backpropagation\n",
    "        \"\"\"\n",
    "        # Berechne Ausgabe der versteckten Schicht\n",
    "        self.hidden = self.sigmoid(np.dot(X, self.weights1) + self.bias1)\n",
    "        # Berechne finale Ausgabe\n",
    "        self.output = self.sigmoid(np.dot(self.hidden, self.weights2) + self.bias2)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, X, y, output, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Führt die Backpropagation durch und aktualisiert die Gewichte.\n",
    "        \n",
    "        Parameter:\n",
    "        - X: Eingabedaten\n",
    "        - y: Erwartete Ausgabe\n",
    "        - output: Tatsächliche Ausgabe des Netzwerks\n",
    "        - learning_rate: Lernrate für die Gewichtsaktualisierung\n",
    "        \"\"\"\n",
    "        # Berechne den Fehler zwischen Vorhersage und tatsächlichem Wert\n",
    "        error = y - output\n",
    "        \n",
    "        # Berechne Gradient für die Ausgabeschicht\n",
    "        d_predicted_output = error * self.sigmoid_derivative(output)\n",
    "        \n",
    "        # Berechne Fehler in der versteckten Schicht\n",
    "        error_hidden = d_predicted_output.dot(self.weights2.T)\n",
    "        d_hidden = error_hidden * self.sigmoid_derivative(self.hidden)\n",
    "        \n",
    "        # Aktualisiere die Gewichte\n",
    "        self.weights2 += self.hidden.T.dot(d_predicted_output) * learning_rate\n",
    "        self.weights1 += X.T.dot(d_hidden) * learning_rate\n",
    "        \n",
    "        # Aktualisiere die Bias-Terme\n",
    "        self.bias2 += np.sum(d_predicted_output, axis=0) * learning_rate\n",
    "        self.bias1 += np.sum(d_hidden, axis=0) * learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs=10000):\n",
    "        \"\"\"\n",
    "        Trainiert das neuronale Netzwerk.\n",
    "        \n",
    "        Parameter:\n",
    "        - X: Trainingsdaten\n",
    "        - y: Erwartete Ausgaben\n",
    "        - epochs: Anzahl der Trainingsdurchläufe\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Vorwärtsdurchlauf\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.backward(X, y, output)\n",
    "            \n",
    "            # Zeige Fortschritt alle 1000 Epochen\n",
    "            if epoch % 1000 == 0:\n",
    "                loss = np.mean(np.square(y - output))\n",
    "                print(f'Epoche {epoch}, Verlust: {loss}')\n",
    "\n",
    "# Beispiel: XOR-Problem\n",
    "if __name__ == \"__main__\":\n",
    "    # Trainingsdaten für das XOR-Problem\n",
    "    X = np.array([[0, 0],  # Eingabe 1\n",
    "                  [0, 1],  # Eingabe 2\n",
    "                  [1, 0],  # Eingabe 3\n",
    "                  [1, 1]]) # Eingabe 4\n",
    "    \n",
    "    y = np.array([[0],     # Erwartete Ausgabe für Eingabe 1\n",
    "                  [1],     # Erwartete Ausgabe für Eingabe 2\n",
    "                  [1],     # Erwartete Ausgabe für Eingabe 3\n",
    "                  [0]])    # Erwartete Ausgabe für Eingabe 4\n",
    "\n",
    "    # Erstelle und trainiere das Netzwerk\n",
    "    print(\"Starte Training...\")\n",
    "    nn = SimpleNeuralNetwork()\n",
    "    nn.train(X, y)\n",
    "    \n",
    "    # Teste das trainierte Netzwerk\n",
    "    print(\"\\nTeste das trainierte Netzwerk:\")\n",
    "    for i in range(len(X)):\n",
    "        prediction = nn.forward(X[i:i+1])\n",
    "        print(f\"Eingabe: {X[i]}, Erwartete Ausgabe: {y[i][0]}, Vorhersage: {prediction[0][0]:.4f}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
