# 6. Neuronale Netze

Neuronale Netze sind ein zentraler Bestandteil moderner KI-Systeme. Sie sind inspiriert vom menschlichen Gehirn, bestehen jedoch aus **kÃ¼nstlichen Neuronen**, die Informationen verarbeiten, weiterleiten und lernen, Muster in Daten zu erkennen.

---

## 6.1 Grundidee kÃ¼nstlicher Neuronen

Ein kÃ¼nstliches Neuron erhÃ¤lt mehrere Eingangswerte, multipliziert sie mit Gewichtungen, summiert die Ergebnisse auf und entscheidet Ã¼ber eine **Aktivierungsfunktion**, ob und wie stark es â€anspringtâ€œ.

### Aufbau eines kÃ¼nstlichen Neurons (Perzeptron):

Das Perzeptron ist das Ã¤lteste und zugleich einfachste Modell eines kÃ¼nstlichen Neurons. Es wurde Ende der 1950er Jahre von Frank Rosenblatt entwickelt und kann lineare Entscheidungsgrenzen erlernen. Ein Perzeptron besteht aus:

- **Eingaben:** \( x_1, x_2, ..., x_n \)  
- **Gewichte:** \( w_1, w_2, ..., w_n \)  
- **Berechnung:**  
 z =   âˆ‘ wi xi + b (mit Bias b) wobei \( b \) der **Bias** ist

- **Aktivierung:** Einer Aktivierungsfunktion (z.B. ein Schwellenwert), die entscheidet, ob das Neuron â€feuertâ€œ


Das Perzeptron lernt durch Anpassung der Gewichte, ob ein Datenpunkt zur Klasse 1 oder 0 gehÃ¶rt. Es kann jedoch nur linear separierbare Probleme lÃ¶sen.

---

## Netzstruktur: Von der Eingabe zur Vorhersage

Ein einzelnes Neuron ist nicht ausreichend fÃ¼r komplexe Aufgaben. Erst durch das **VerknÃ¼pfen mehrerer Neuronen zu Netzwerken** entstehen leistungsfÃ¤hige Strukturen:

### Typische Architektur eines neuronalen Netzes:

- **Eingabeschicht:** Nimmt Rohdaten auf (z.â€¯B. Pixel, Sensordaten)
- **Versteckte Schichten (Hidden Layers):** Transformieren und abstrahieren Merkmale
- **Ausgabeschicht:** Gibt die finale Vorhersage aus (z.â€¯B. Kategorie, Wert)

Netze mit mehreren versteckten Schichten werden als **Mehrschicht-Perzeptrons (MLP)** bezeichnet. Sie gehÃ¶ren zur Klasse der **Feedforward-Netze**, bei denen Informationen nur in eine Richtung flieÃŸen â€“ von der Eingabe zur Ausgabe.

![image](https://github.com/user-attachments/assets/8d540b6c-2be0-49f7-bbd7-b2d951789290)

*Feedforward-Netz mit 2 Hidden Layers â€“ Quelle: ORDIX Blog, 2021*


**ErklÃ¤rung:**  

Die Eingabeschicht (blau) nimmt die Merkmale x1 bis x5 auf. Diese werden durch zehn Neuronen in der ersten verdeckten Schicht verarbeitet, anschlieÃŸend durch fÃ¼nf weitere Neuronen in der zweiten versteckten Schicht weitergegeben und schlieÃŸlich zu einer Vorhersage y1 in der Ausgabeschicht (grÃ¼n) zusammengefÃ¼hrt. Jede Verbindung ist mit einem Gewicht versehen, das beim Lernen angepasst wird.

---

## Anwendung: Bilderkennung

Ein neuronales Netz kann z.â€¯B. lernen, **handgeschriebene Ziffern (0â€“9)** zu erkennen:

1. **Eingabeschicht:** nimmt Pixelwerte des Bildes auf  
2. **Versteckte Schichten:** extrahieren Muster wie Kanten oder Kurven  
3. **Ausgabeschicht:** entscheidet sich fÃ¼r die wahrscheinlichste Ziffer

---

> Ein neuronales Netz lernt durch viele Beispiele, relevante Merkmale in den Daten zu erkennen â€“ ganz ohne explizit programmierte Regeln.



## 6.2 Lernen mit Backpropagation

Damit ein neuronales Netz nicht nur zufÃ¤llige Ausgaben produziert, sondern gezielt Muster erkennen kann, muss es aus Beispielen lernen. Das zentrale Trainingsverfahren fÃ¼r Feedforward-Netze ist der **Backpropagation-Algorithmus** â€“ auf Deutsch: RÃ¼ckwÃ¤rtsausbreitung des Fehlers.

### Ziel: Fehler minimieren

Der Lernprozess basiert auf dem Vergleich zwischen der tatsÃ¤chlichen Ausgabe \( \hat{y} \) des Netzes und dem Zielwert \( y \). Die Differenz wird durch eine **Fehlerfunktion (Loss Function)** quantifiziert, z.â€¯B.:

![image](https://github.com/user-attachments/assets/71d425bc-95e2-435b-ad2a-7f666f5ab9ad)


Ziel ist es, die Summe der Fehler Ã¼ber alle Trainingsbeispiele hinweg zu minimieren. Dazu passt das Netz seine **Gewichte** iterativ an.

---

### Idee der RÃ¼ckwÃ¤rtsausbreitung

Backpropagation nutzt die **Kettenregel** der Differentialrechnung, um zu berechnen, wie stark jedes Gewicht zum Fehler beigetragen hat. Der Fehler wird dabei von der **Ausgabe zurÃ¼ck zur Eingabe** propagiert.
Jedes Gewicht \( w \) erhÃ¤lt ein individuelles **GradientenmaÃŸ**, das angibt, in welche Richtung es angepasst werden sollte.

Der Anpassungsschritt erfolgt dann mit Hilfe des Gradientenabstiegs:

![image](https://github.com/user-attachments/assets/9eedce74-ca79-4b24-8250-191afc1636d0)

Dabei ist Î· die Lernrate, die steuert, wie stark die Gewichte verÃ¤ndert werden. Ist sie zu groÃŸ, â€springtâ€œ das Netz Ã¼ber das Minimum hinaus; ist sie zu klein, dauert das Lernen sehr lange.

![image](https://github.com/user-attachments/assets/7c2fa529-b1e3-4d97-85d4-182e51366c2a)

*Abbildung: Backpropagation in einem neuronalen Netz â€“ Quelle: GeeksforGeeks (2023)*

---

### Beispiel: Spam-Klassifikation mit einem MLP

Ein **Mehrschicht-Perzeptron (MLP)** soll erkennen, ob eine E-Mail Spam ist. In der Trainingsphase erhÃ¤lt das Netz hunderte Beispiele mit Labels:

- â€Spamâ€œ  
- â€Kein Spamâ€œ

Der Ablauf:

1. Vorhersage durch das Netz  
2. Vergleich mit dem Label  
3. Fehlerberechnung  
4. Fehler wird durch das Netz zurÃ¼ckgeleitet  
5. Gewichte werden aktualisiert

Mit jeder Iteration verbessert sich die Erkennung. Das Netz lernt, welche Merkmalskombinationen typisch fÃ¼r Spam sind â€“ z.â€¯B. bestimmte WÃ¶rter im Betreff.

---

### Bedeutung fÃ¼r moderne Netze

**Backpropagation** ist das HerzstÃ¼ck nahezu aller modernen neuronalen Netze:

- **MLPs**
- **Convolutional Neural Networks (CNNs)**
- **Recurrent Neural Networks (RNNs)**
- u.â€¯v.â€¯m.

Ohne diesen Algorithmus wÃ¤re das effiziente Training tiefer Modelle kaum mÃ¶glich.

### Erweiterte Optimierungsstrategien

Zur Verbesserung des Lernverhaltens kommen hÃ¤ufig zusÃ¤tzliche Methoden zum Einsatz:

- **Momentum:** beschleunigt den Lernprozess  
- **Adam (Adaptive Moment Estimation):** kombiniert Momentum und RMSprop  
- **Early Stopping:** verhindert Ãœberanpassung durch Abbruch bei stagnierender Validierungsleistung

> Backpropagation + geeignete Optimierungsstrategien = Grundlage moderner Deep Learning Systeme


## 6.3 Tiefe neuronale Netze (Deep Neural Networks)

Tiefe neuronale Netze â€“ hÃ¤ufig als **Deep Neural Networks (DNNs)** bezeichnet â€“ bilden das HerzstÃ¼ck moderner Deep-Learning-Anwendungen. Sie erweitern klassische kÃ¼nstliche neuronale Netze (ANNs) um eine **deutlich grÃ¶ÃŸere Anzahl an versteckten Schichten (Hidden Layers)**.

WÃ¤hrend einfache Feedforward-Netze meist nur eine oder zwei versteckte Schichten besitzen, verwenden DNNs oft **dutzende Layer**, um zunehmend komplexere Merkmale zu lernen.

---

### Merkmale tiefer Netze

- **Hierarchisches Lernen:**  
  FrÃ¼here Schichten lernen einfache Merkmale (z.â€¯B. Kanten, Farben). SpÃ¤tere Schichten kombinieren diese zu komplexeren ReprÃ¤sentationen (z.â€¯B. Gesichter, Objekte, Sprache).

- **NichtlinearitÃ¤t durch Aktivierungsfunktionen:**  
  HÃ¤ufig eingesetzt: **ReLU (Rectified Linear Unit)**  
  Vorteile:
  - Vermeidet das Problem des **verschwindenden Gradienten**
  - FÃ¼hrt zu schnellerer Konvergenz beim Lernen

- **Hoher Bedarf an Rechenleistung und Daten:**  
  - Erfordert **groÃŸe Mengen gelabelter Daten** (z.â€¯B. Millionen Bilder)
  - Training erfolgt auf leistungsstarker Hardware: **GPU** oder **TPU**

---

### Vergleich: Konventionelles ML vs. Deep Learning vs. LLMs

| Merkmal                  | Traditionelles ML | Deep Learning (DNNs) | LLMs (z.â€¯B. ChatGPT) |
|--------------------------|-------------------|------------------------|-----------------------|
| Trainingsdatenmenge      | GroÃŸ              | GroÃŸ                   | Sehr groÃŸ             |
| Feature Engineering      | Manuell           | Automatisch            | Automatisch           |
| ModellkomplexitÃ¤t        | Begrenzt          | Hoch                   | Extrem hoch           |
| Interpretierbarkeit      | Gut               | Schwach                | Sehr schwach          |
| Leistung                 | MittelmÃ¤ÃŸig       | Hoch                   | Sehr hoch             |
| Hardwareanforderungen    | Gering            | Hoch                   | Sehr hoch             |

*Quelle: 430_DL_DNN, Seite 24*

---

### Anwendungen tiefer Netze

DNNs kommen in vielen Bereichen zum Einsatz:

- ğŸ—£ **Sprachverarbeitung**  
  *Beispiele:* Google Translate, ChatGPT, Alexa

- ğŸ©º **Medizinische Bildanalyse**  
  *Beispiel:* Erkennung von Tumoren oder Anomalien in RÃ¶ntgenbildern

- ğŸ’³ **Finanzwesen**  
  *Beispiel:* Betrugserkennung bei Kreditkartentransaktionen

- ğŸš— **Autonomes Fahren**  
  *Beispiel:* Bilderkennung, Objekterkennung, Sensorfusion

- ğŸ“· **Bild- und Objekterkennung**  
  *Beispiel:* Klassifikation in der Industrie, Sicherheitsanwendungen

---

> Tiefe neuronale Netze sind heute aus der KI nicht mehr wegzudenken â€“ sie liefern die Grundlage fÃ¼r viele der fortschrittlichsten Technologien unserer Zeit.

## 6.4 Convolutional Neural Networks (CNNs)

**Convolutional Neural Networks (CNNs)** sind speziell fÃ¼r die Verarbeitung visueller Daten (z.â€¯B. Bilder, Videos) entwickelte tiefe neuronale Netze. Ihre Architektur ermÃ¶glicht es, **lokale visuelle Merkmale** automatisch zu erkennen und schrittweise zu abstrahieren â€“ ideal fÃ¼r Klassifikations-, Erkennungs- und Segmentierungsaufgaben.

Im Gegensatz zu vollstÃ¤ndig verbundenen Netzen nutzen CNNs **Faltungsschichten (Convolutional Layers)** und **Pooling-Schichten**, wodurch die Anzahl der Parameter und die KomplexitÃ¤t reduziert wird.

---

### Architektur und Datenfluss

![image](https://github.com/user-attachments/assets/5c88fbfa-d8b1-4841-8480-c0872d347367)
 
*Abbildung: CNN-Aufbau von der Eingabe bis zur Klassifikation â€“ Quelle: MathWorks*

**Ablauf eines CNNs:**

1. **Convolutional Layer:**  
   Extrahiert lokale Merkmale (z.â€¯B. Kanten, Texturen) mithilfe von Filtern

2. **ReLU-Aktivierung:**  
   FÃ¼hrt NichtlinearitÃ¤t ein und hilft beim Training tiefere Strukturen zu lernen

3. **Pooling Layer:**  
   Reduziert die DimensionalitÃ¤t (z.â€¯B. mit MaxPooling) und erhÃ¶ht die Translationstoleranz

4. **Wiederholung von Convolution + Pooling:**  
   Merkmale werden hierarchisch komplexer â€“ von Linien zu ganzen Objekten

5. **Flatten Layer:**  
   Wandelt Merkmalskarten in einen Vektor um

6. **Dense (Fully Connected) Layer:**  
   VerknÃ¼pft alle Neuronen zur Entscheidungsbildung

7. **Softmax-Ausgabe:**  
   Gibt Wahrscheinlichkeiten fÃ¼r jede Klasse aus (z.â€¯B. â€Autoâ€œ, â€LKWâ€œ, â€Fahrradâ€œ)

---

### Feature Learning & Klassifikation

Die CNN-Architektur lÃ¤sst sich in zwei Hauptphasen gliedern:

- **Feature Learning:**  
  Merkmale werden automatisch erkannt â€“ vom Pixel Ã¼ber Kanten bis zu komplexen Objekten

- **Classification:**  
  In der vollstÃ¤ndig verbundenen Endphase wird entschieden, zu welcher Klasse das Bild gehÃ¶rt â€“ meist durch **Softmax**.

---

### Vorteile von CNNs

âœ… **LokalitÃ¤t:** Durch Filter beschrÃ¤nkt auf kleine Bildausschnitte  
âœ… **Parameterersparnis:** Gewichtsteilung reduziert Rechenaufwand  
âœ… **Translationstoleranz:** Pooling macht das Netz robust gegenÃ¼ber Objektverschiebung  
âœ… **Hohe Genauigkeit:** Besonders bei Bildklassifikation & Objekterkennung

---

### Typische Einsatzgebiete

- ğŸ“¸ Bildklassifikation (z.â€¯B. ImageNet)
- ğŸ” Objekterkennung (z.â€¯B. YOLO, Faster R-CNN)
- ğŸ§  Medizinische Bilddiagnostik (z.â€¯B. Tumorerkennung)
- ğŸš— Autonomes Fahren (z.â€¯B. Verkehrsschilderkennung)
- ğŸ“¹ Videoanalyse & Gesichtserkennung

---

## Quellen

1. ORDIX Blog (2021): Einstieg in neuronale Netze mit TensorFlow und Keras  
   [https://blog.ordix.de/einstieg-in-neuronale-netze-mit-tensorflow-und-keras](https://blog.ordix.de/einstieg-in-neuronale-netze-mit-tensorflow-und-keras)

2. Rosenblatt, F. (1958): *The Perceptron*  
   Psychological Review, Vol. 65, No. 6

3. Goodfellow, I., Bengio, Y., Courville, A. (2016): *Deep Learning*  
   [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)

4. Mitchell, T. (1997): *Machine Learning*, McGraw-Hill Education

5. Kriegel, H.-P. et al. (2020): *KÃ¼nstliche Intelligenz: Grundlagen intelligenter Systeme*, Springer Vieweg

6. Wikipedia (2025): [KÃ¼nstliches neuronales Netz](https://de.wikipedia.org/wiki/KÃ¼nstliches_neuronales_Netz)

7. GeeksforGeeks (2023): [Backpropagation in Neural Network](https://www.geeksforgeeks.org/backpropagation-in-neural-network/)

