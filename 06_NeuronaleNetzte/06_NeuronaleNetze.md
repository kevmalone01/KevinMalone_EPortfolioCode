# 6. Neuronale Netze

Neuronale Netze sind ein zentraler Bestandteil moderner KI-Systeme. Sie sind inspiriert vom menschlichen Gehirn, bestehen jedoch aus **k√ºnstlichen Neuronen**, die Informationen verarbeiten, weiterleiten und lernen, Muster in Daten zu erkennen.

---

## 6.1 Grundidee k√ºnstlicher Neuronen

Ein k√ºnstliches Neuron erh√§lt mehrere Eingangswerte, multipliziert sie mit Gewichtungen, summiert die Ergebnisse auf und entscheidet √ºber eine **Aktivierungsfunktion**, ob und wie stark es ‚Äûanspringt‚Äú.

### Aufbau eines k√ºnstlichen Neurons (Perzeptron):

Das Perzeptron ist das √§lteste und zugleich einfachste Modell eines k√ºnstlichen Neurons. Es wurde Ende der 1950er Jahre von Frank Rosenblatt entwickelt und kann lineare Entscheidungsgrenzen erlernen. Ein Perzeptron besteht aus:

- **Eingaben:** \( x_1, x_2, ..., x_n \)  
- **Gewichte:** \( w_1, w_2, ..., w_n \)  
- **Berechnung:**  
 z =   ‚àë wi xi + b (mit Bias b) wobei \( b \) der **Bias** ist

- **Aktivierung:** Einer Aktivierungsfunktion (z.B. ein Schwellenwert), die entscheidet, ob das Neuron ‚Äûfeuert‚Äú


Das Perzeptron lernt durch Anpassung der Gewichte, ob ein Datenpunkt zur Klasse 1 oder 0 geh√∂rt. Es kann jedoch nur linear separierbare Probleme l√∂sen.

---

## Netzstruktur: Von der Eingabe zur Vorhersage

Ein einzelnes Neuron ist nicht ausreichend f√ºr komplexe Aufgaben. Erst durch das **Verkn√ºpfen mehrerer Neuronen zu Netzwerken** entstehen leistungsf√§hige Strukturen:

### Typische Architektur eines neuronalen Netzes:

- **Eingabeschicht:** Nimmt Rohdaten auf (z.‚ÄØB. Pixel, Sensordaten)
- **Versteckte Schichten (Hidden Layers):** Transformieren und abstrahieren Merkmale
- **Ausgabeschicht:** Gibt die finale Vorhersage aus (z.‚ÄØB. Kategorie, Wert)

Netze mit mehreren versteckten Schichten werden als **Mehrschicht-Perzeptrons (MLP)** bezeichnet. Sie geh√∂ren zur Klasse der **Feedforward-Netze**, bei denen Informationen nur in eine Richtung flie√üen ‚Äì von der Eingabe zur Ausgabe.

![image](https://github.com/user-attachments/assets/8d540b6c-2be0-49f7-bbd7-b2d951789290)

*Feedforward-Netz mit 2 Hidden Layers ‚Äì Quelle: ORDIX Blog, 2021*


**Erkl√§rung:**  

Die Eingabeschicht (blau) nimmt die Merkmale x1 bis x5 auf. Diese werden durch zehn Neuronen in der ersten verdeckten Schicht verarbeitet, anschlie√üend durch f√ºnf weitere Neuronen in der zweiten versteckten Schicht weitergegeben und schlie√ülich zu einer Vorhersage y1 in der Ausgabeschicht (gr√ºn) zusammengef√ºhrt. Jede Verbindung ist mit einem Gewicht versehen, das beim Lernen angepasst wird.

---

## Anwendung: Bilderkennung

Ein neuronales Netz kann z.‚ÄØB. lernen, **handgeschriebene Ziffern (0‚Äì9)** zu erkennen:

1. **Eingabeschicht:** nimmt Pixelwerte des Bildes auf  
2. **Versteckte Schichten:** extrahieren Muster wie Kanten oder Kurven  
3. **Ausgabeschicht:** entscheidet sich f√ºr die wahrscheinlichste Ziffer

---

> Ein neuronales Netz lernt durch viele Beispiele, relevante Merkmale in den Daten zu erkennen ‚Äì ganz ohne explizit programmierte Regeln.



## 6.2 Lernen mit Backpropagation

Damit ein neuronales Netz nicht nur zuf√§llige Ausgaben produziert, sondern gezielt Muster erkennen kann, muss es aus Beispielen lernen. Das zentrale Trainingsverfahren f√ºr Feedforward-Netze ist der **Backpropagation-Algorithmus** ‚Äì auf Deutsch: R√ºckw√§rtsausbreitung des Fehlers.

### Ziel: Fehler minimieren

Der Lernprozess basiert auf dem Vergleich zwischen der tats√§chlichen Ausgabe \( \hat{y} \) des Netzes und dem Zielwert \( y \). Die Differenz wird durch eine **Fehlerfunktion (Loss Function)** quantifiziert, z.‚ÄØB.:

![image](https://github.com/user-attachments/assets/71d425bc-95e2-435b-ad2a-7f666f5ab9ad)


Ziel ist es, die Summe der Fehler √ºber alle Trainingsbeispiele hinweg zu minimieren. Dazu passt das Netz seine **Gewichte** iterativ an.

---

### Idee der R√ºckw√§rtsausbreitung

Backpropagation nutzt die **Kettenregel** der Differentialrechnung, um zu berechnen, wie stark jedes Gewicht zum Fehler beigetragen hat. Der Fehler wird dabei von der **Ausgabe zur√ºck zur Eingabe** propagiert.
Jedes Gewicht \( w \) erh√§lt ein individuelles **Gradientenma√ü**, das angibt, in welche Richtung es angepasst werden sollte.

Der Anpassungsschritt erfolgt dann mit Hilfe des Gradientenabstiegs:

![image](https://github.com/user-attachments/assets/9eedce74-ca79-4b24-8250-191afc1636d0)

Dabei ist Œ∑ die Lernrate, die steuert, wie stark die Gewichte ver√§ndert werden. Ist sie zu gro√ü, ‚Äûspringt‚Äú das Netz √ºber das Minimum hinaus; ist sie zu klein, dauert das Lernen sehr lange.

![image](https://github.com/user-attachments/assets/7c2fa529-b1e3-4d97-85d4-182e51366c2a)

*Abbildung: Backpropagation in einem neuronalen Netz ‚Äì Quelle: GeeksforGeeks (2023)*

---

### Beispiel: Spam-Klassifikation mit einem MLP

Ein **Mehrschicht-Perzeptron (MLP)** soll erkennen, ob eine E-Mail Spam ist. In der Trainingsphase erh√§lt das Netz hunderte Beispiele mit Labels:

- ‚ÄûSpam‚Äú  
- ‚ÄûKein Spam‚Äú

Der Ablauf:

1. Vorhersage durch das Netz  
2. Vergleich mit dem Label  
3. Fehlerberechnung  
4. Fehler wird durch das Netz zur√ºckgeleitet  
5. Gewichte werden aktualisiert

Mit jeder Iteration verbessert sich die Erkennung. Das Netz lernt, welche Merkmalskombinationen typisch f√ºr Spam sind ‚Äì z.‚ÄØB. bestimmte W√∂rter im Betreff.

---

### Bedeutung f√ºr moderne Netze

**Backpropagation** ist das Herzst√ºck nahezu aller modernen neuronalen Netze:

- **MLPs**
- **Convolutional Neural Networks (CNNs)**
- **Recurrent Neural Networks (RNNs)**
- u.‚ÄØv.‚ÄØm.

Ohne diesen Algorithmus w√§re das effiziente Training tiefer Modelle kaum m√∂glich.

### Erweiterte Optimierungsstrategien

Zur Verbesserung des Lernverhaltens kommen h√§ufig zus√§tzliche Methoden zum Einsatz:

- **Momentum:** beschleunigt den Lernprozess  
- **Adam (Adaptive Moment Estimation):** kombiniert Momentum und RMSprop  
- **Early Stopping:** verhindert √úberanpassung durch Abbruch bei stagnierender Validierungsleistung

> Backpropagation + geeignete Optimierungsstrategien = Grundlage moderner Deep Learning Systeme


## 6.3 Tiefe neuronale Netze (Deep Neural Networks)

Tiefe neuronale Netze ‚Äì h√§ufig als **Deep Neural Networks (DNNs)** bezeichnet ‚Äì bilden das Herzst√ºck moderner Deep-Learning-Anwendungen. Sie erweitern klassische k√ºnstliche neuronale Netze (ANNs) um eine **deutlich gr√∂√üere Anzahl an versteckten Schichten (Hidden Layers)**.

W√§hrend einfache Feedforward-Netze meist nur eine oder zwei versteckte Schichten besitzen, verwenden DNNs oft **dutzende Layer**, um zunehmend komplexere Merkmale zu lernen.

---

### Merkmale tiefer Netze

- **Hierarchisches Lernen:**  
  Fr√ºhere Schichten lernen einfache Merkmale (z.‚ÄØB. Kanten, Farben). Sp√§tere Schichten kombinieren diese zu komplexeren Repr√§sentationen (z.‚ÄØB. Gesichter, Objekte, Sprache).

- **Nichtlinearit√§t durch Aktivierungsfunktionen:**  
  H√§ufig eingesetzt: **ReLU (Rectified Linear Unit)**  
  Vorteile:
  - Vermeidet das Problem des **verschwindenden Gradienten**
  - F√ºhrt zu schnellerer Konvergenz beim Lernen

- **Hoher Bedarf an Rechenleistung und Daten:**  
  - Erfordert **gro√üe Mengen gelabelter Daten** (z.‚ÄØB. Millionen Bilder)
  - Training erfolgt auf leistungsstarker Hardware: **GPU** oder **TPU**

---

### Vergleich: Konventionelles ML vs. Deep Learning vs. LLMs

| Merkmal                  | Traditionelles ML | Deep Learning (DNNs) | LLMs (z.‚ÄØB. ChatGPT) |
|--------------------------|-------------------|------------------------|-----------------------|
| Trainingsdatenmenge      | Gro√ü              | Gro√ü                   | Sehr gro√ü             |
| Feature Engineering      | Manuell           | Automatisch            | Automatisch           |
| Modellkomplexit√§t        | Begrenzt          | Hoch                   | Extrem hoch           |
| Interpretierbarkeit      | Gut               | Schwach                | Sehr schwach          |
| Leistung                 | Mittelm√§√üig       | Hoch                   | Sehr hoch             |
| Hardwareanforderungen    | Gering            | Hoch                   | Sehr hoch             |

*Quelle: 430_DL_DNN, Seite 24*

---

### Anwendungen tiefer Netze

DNNs kommen in vielen Bereichen zum Einsatz:

- üó£ **Sprachverarbeitung**  
  *Beispiele:* Google Translate, ChatGPT, Alexa

- ü©∫ **Medizinische Bildanalyse**  
  *Beispiel:* Erkennung von Tumoren oder Anomalien in R√∂ntgenbildern

- üí≥ **Finanzwesen**  
  *Beispiel:* Betrugserkennung bei Kreditkartentransaktionen

- üöó **Autonomes Fahren**  
  *Beispiel:* Bilderkennung, Objekterkennung, Sensorfusion

- üì∑ **Bild- und Objekterkennung**  
  *Beispiel:* Klassifikation in der Industrie, Sicherheitsanwendungen

---

> Tiefe neuronale Netze sind heute aus der KI nicht mehr wegzudenken ‚Äì sie liefern die Grundlage f√ºr viele der fortschrittlichsten Technologien unserer Zeit.

## 6.4 Convolutional Neural Networks (CNNs)

**Convolutional Neural Networks (CNNs)** sind speziell f√ºr die Verarbeitung visueller Daten (z.‚ÄØB. Bilder, Videos) entwickelte tiefe neuronale Netze. Ihre Architektur erm√∂glicht es, **lokale visuelle Merkmale** automatisch zu erkennen und schrittweise zu abstrahieren ‚Äì ideal f√ºr Klassifikations-, Erkennungs- und Segmentierungsaufgaben.

Im Gegensatz zu vollst√§ndig verbundenen Netzen nutzen CNNs **Faltungsschichten (Convolutional Layers)** und **Pooling-Schichten**, wodurch die Anzahl der Parameter und die Komplexit√§t reduziert wird.

---

### Architektur und Datenfluss

![image](https://github.com/user-attachments/assets/5c88fbfa-d8b1-4841-8480-c0872d347367)
 
*Abbildung: CNN-Aufbau von der Eingabe bis zur Klassifikation ‚Äì Quelle: MathWorks*

**Ablauf eines CNNs:**

1. **Convolutional Layer:**  
   Extrahiert lokale Merkmale (z.‚ÄØB. Kanten, Texturen) mithilfe von Filtern

2. **ReLU-Aktivierung:**  
   F√ºhrt Nichtlinearit√§t ein und hilft beim Training tiefere Strukturen zu lernen

3. **Pooling Layer:**  
   Reduziert die Dimensionalit√§t (z.‚ÄØB. mit MaxPooling) und erh√∂ht die Translationstoleranz

4. **Wiederholung von Convolution + Pooling:**  
   Merkmale werden hierarchisch komplexer ‚Äì von Linien zu ganzen Objekten

5. **Flatten Layer:**  
   Wandelt Merkmalskarten in einen Vektor um

6. **Dense (Fully Connected) Layer:**  
   Verkn√ºpft alle Neuronen zur Entscheidungsbildung

7. **Softmax-Ausgabe:**  
   Gibt Wahrscheinlichkeiten f√ºr jede Klasse aus (z.‚ÄØB. ‚ÄûAuto‚Äú, ‚ÄûLKW‚Äú, ‚ÄûFahrrad‚Äú)

---

### Feature Learning & Klassifikation

Die CNN-Architektur l√§sst sich in zwei Hauptphasen gliedern:

- **Feature Learning:**  
  Merkmale werden automatisch erkannt ‚Äì vom Pixel √ºber Kanten bis zu komplexen Objekten

- **Classification:**  
  In der vollst√§ndig verbundenen Endphase wird entschieden, zu welcher Klasse das Bild geh√∂rt ‚Äì meist durch **Softmax**.

---

### Vorteile von CNNs

‚úÖ **Lokalit√§t:** Durch Filter beschr√§nkt auf kleine Bildausschnitte  
‚úÖ **Parameterersparnis:** Gewichtsteilung reduziert Rechenaufwand  
‚úÖ **Translationstoleranz:** Pooling macht das Netz robust gegen√ºber Objektverschiebung  
‚úÖ **Hohe Genauigkeit:** Besonders bei Bildklassifikation & Objekterkennung

---

### Typische Einsatzgebiete

- üì∏ Bildklassifikation (z.‚ÄØB. ImageNet)
- üîç Objekterkennung (z.‚ÄØB. YOLO, Faster R-CNN)
- üß† Medizinische Bilddiagnostik (z.‚ÄØB. Tumorerkennung)
- üöó Autonomes Fahren (z.‚ÄØB. Verkehrsschilderkennung)
- üìπ Videoanalyse & Gesichtserkennung

---

## Quellen

1. ORDIX Blog (2021): Einstieg in neuronale Netze mit TensorFlow und Keras  
   [https://blog.ordix.de/einstieg-in-neuronale-netze-mit-tensorflow-und-keras](https://blog.ordix.de/einstieg-in-neuronale-netze-mit-tensorflow-und-keras)

2. Rosenblatt, F. (1958): *The Perceptron*  
   Psychological Review, Vol. 65, No. 6

3. Goodfellow, I., Bengio, Y., Courville, A. (2016): *Deep Learning*  
   [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)

4. Mitchell, T. (1997): *Machine Learning*, McGraw-Hill Education

5. Kriegel, H.-P. et al. (2020): *K√ºnstliche Intelligenz: Grundlagen intelligenter Systeme*, Springer Vieweg

6. Wikipedia (2025): [K√ºnstliches neuronales Netz](https://de.wikipedia.org/wiki/K√ºnstliches_neuronales_Netz)

7. GeeksforGeeks (2023): [Backpropagation in Neural Network](https://www.geeksforgeeks.org/backpropagation-in-neural-network/)

