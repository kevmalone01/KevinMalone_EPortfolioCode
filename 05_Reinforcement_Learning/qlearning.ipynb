{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9e3771",
   "metadata": {},
   "source": [
    "Q-Learning Minimal Beispiel\n",
    "===========================\n",
    "\n",
    "Dieses Programm demonstriert Q-Learning, eine grundlegende Methode des Reinforcement Learning,\n",
    "in ihrer einfachsten Form. Es zeigt, wie ein Agent lernt, sich in einer linearen Umgebung\n",
    "von einem Startpunkt zu einem Ziel zu bewegen.\n",
    "\n",
    "Die Umgebung:\n",
    "------------\n",
    "- 5 Zustände (0 bis 4)\n",
    "- 2 mögliche Aktionen: Vorwärts (1) oder Rückwärts (0)\n",
    "- Belohnung: 1 im Zielzustand (4), 0 sonst\n",
    "- Der Agent startet immer bei 0 und soll zu 4 kommen\n",
    "\n",
    "Wie es funktioniert:\n",
    "------------------\n",
    "1. Der Agent hat eine Q-Tabelle, die für jeden Zustand und jede Aktion einen Wert speichert\n",
    "2. In jedem Schritt:\n",
    "   - Wählt der Agent eine Aktion (manchmal zufällig, meist die beste)\n",
    "   - Führt die Aktion aus und bekommt eine Belohnung\n",
    "   - Aktualisiert seine Q-Tabelle basierend auf der Erfahrung\n",
    "3. Nach vielen Episoden hat der Agent gelernt, den optimalen Pfad zu finden\n",
    "\n",
    "Die Parameter:\n",
    "-------------\n",
    "- alpha (0.1): Lernrate - wie stark der Agent aus jeder Erfahrung lernt\n",
    "- gamma (0.9): Diskontfaktor - wie wichtig zukünftige Belohnungen sind\n",
    "- epsilon (0.2): Zufallsrate - wie oft der Agent neue Aktionen ausprobiert\n",
    "- episodes (200): Anzahl der Übungsepisoden\n",
    "\n",
    "Ausgabe:\n",
    "--------\n",
    "- Die finale Q-Tabelle zeigt die gelernten Werte für jede Aktion in jedem Zustand\n",
    "- Der optimale Pfad zeigt den besten Weg vom Start zum Ziel\n",
    "\n",
    "Beispiel:\n",
    "---------\n",
    "Zustand 0: [0.61, 0.73]  # [Rückwärts, Vorwärts]\n",
    "Zustand 1: [0.57, 0.81]\n",
    "Zustand 2: [0.64, 0.90]\n",
    "Zustand 3: [0.70, 1.00]\n",
    "Zustand 4: [0.00, 0.00]\n",
    "\n",
    "Optimaler Pfad: 0 -> 1 -> 2 -> 3 -> 4\n",
    "\n",
    "Die höheren Werte in der zweiten Spalte zeigen, dass \"Vorwärts\" in jedem Zustand\n",
    "die bessere Aktion ist, und die Werte steigen, je näher man dem Ziel kommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989c545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Tabelle:\n",
      "Zustand 0: Rückwärts=0.55, Vorwärts=0.73\n",
      "Zustand 1: Rückwärts=0.55, Vorwärts=0.81\n",
      "Zustand 2: Rückwärts=0.67, Vorwärts=0.90\n",
      "Zustand 3: Rückwärts=0.77, Vorwärts=1.00\n",
      "Zustand 4: Rückwärts=0.00, Vorwärts=0.00\n",
      "\n",
      "Optimaler Pfad von 0 nach 4:\n",
      "0 -> 1 -> 2 -> 3 -> 4\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Parameter\n",
    "num_states = 5    # Anzahl der Zustände (0 bis 4)\n",
    "num_actions = 2   # Anzahl der Aktionen (0 = rückwärts, 1 = vorwärts)\n",
    "alpha = 0.1       # Lernrate (wie schnell lernt der Agent?)\n",
    "gamma = 0.9       # Diskontfaktor (wie wichtig sind zukünftige Belohnungen?)\n",
    "epsilon = 0.2     # Zufallsrate (wie oft probiert der Agent zufällige Aktionen?)\n",
    "episodes = 200    # Anzahl der Übungsepisoden\n",
    "\n",
    "q_table = [[0.0 for _ in range(num_actions)] for _ in range(num_states)]\n",
    "\n",
    "def step(state, action):\n",
    "    \"\"\"Simuliert die Umgebung: gibt neuen Zustand und Belohnung zurück.\"\"\"\n",
    "    if action == 1:  # vorwärts\n",
    "        next_state = min(state + 1, num_states - 1)\n",
    "    else:            # rückwärts\n",
    "        next_state = max(state - 1, 0)\n",
    "    reward = 1 if next_state == num_states - 1 else 0\n",
    "    return next_state, reward\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = 0\n",
    "    for _ in range(20):  # max. 20 Schritte pro Episode\n",
    "        # Aktion wählen (Epsilon-Greedy)\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice([0, 1])\n",
    "        else:\n",
    "            action = 0 if q_table[state][0] > q_table[state][1] else 1\n",
    "\n",
    "        next_state, reward = step(state, action)\n",
    "        # Q-Learning Update\n",
    "        best_next = max(q_table[next_state])\n",
    "        q_table[state][action] += alpha * (reward + gamma * best_next - q_table[state][action])\n",
    "        state = next_state\n",
    "        if state == num_states - 1:\n",
    "            break\n",
    "\n",
    "# Zeige die gelernten Q-Werte und den optimalen Pfad\n",
    "print(\"Q-Tabelle:\")\n",
    "for s in range(num_states):\n",
    "    print(f\"Zustand {s}: Rückwärts={q_table[s][0]:.2f}, Vorwärts={q_table[s][1]:.2f}\")\n",
    "\n",
    "print(\"\\nOptimaler Pfad von 0 nach 4:\")\n",
    "state = 0\n",
    "path = [state]\n",
    "while state != num_states - 1:\n",
    "    action = 0 if q_table[state][0] > q_table[state][1] else 1\n",
    "    state, _ = step(state, action)\n",
    "    path.append(state)\n",
    "print(\" -> \".join(map(str, path))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
