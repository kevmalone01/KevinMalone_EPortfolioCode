# 5. BestÃ¤rkendes Lernen â€“ Reinforcement Learning

## 5.1 Grundprinzip

Beim **bestÃ¤rkenden Lernen** (Reinforcement Learning, RL) lernt ein Agent durch Interaktion mit seiner Umgebung. Ziel ist es, eine **Strategie (Policy)** zu entwickeln, die langfristig mÃ¶glichst viel Belohnung maximiert.

Nach jeder Aktion erhÃ¤lt der Agent eine RÃ¼ckmeldung (Reward) in Form von Belohnung oder Bestrafung. Durch diese Feedbackschleife wird das Verhalten des Agenten Schritt fÃ¼r Schritt angepasst und verbessert.


![image](https://github.com/user-attachments/assets/11fa8f4b-bf7c-458a-a2f7-620d31529b92)
 
*Abbildung 10: Grundprinzip des Reinforcement Learning - Quelle: DatabaseCamp*

**Ablauf:**
1. Agent beobachtet Zustand \( s \)
2. WÃ¤hlt Aktion \( a \) gemÃ¤ÃŸ seiner aktuellen Policy
3. Umgebung liefert neue Beobachtung \( s' \) und Belohnung \( r \)
4. Policy wird anhand der erhaltenen Belohnung angepasst

---

## 5.2 Q-Learning

Ein grundlegender Algorithmus im RL ist das **Q-Learning**. Dabei wird eine sogenannte **Q-Tabelle** gefÃ¼hrt, in der jeder Zustand \( s \) und jede mÃ¶gliche Aktion \( a \) einen **Q-Wert** zugeordnet bekommt. Dieser gibt an, wie lohnenswert es ist, im Zustand \( s \) die Aktion \( a \) auszufÃ¼hren.

### Q-Update-Regel:

![image](https://github.com/user-attachments/assets/b86108b5-7327-4528-9563-51c85249ac74)


-  Î±:  Lernrate (wie stark neue Informationen zÃ¤hlen)  
-  Î³: Diskontierungsfaktor fÃ¼r zukÃ¼nftige Belohnungen  
-  R: unmittelbare Belohnung  
-  sâ€²: Folgezustand

---

### Beispiele:

#### ðŸ­ Maus im Labyrinth
Eine Maus lernt Ã¼ber viele Versuche, wo sich KÃ¤se (+1) und Fallen (â€“1) befinden. Die Q-Werte helfen ihr, sich kÃ¼nftig fÃ¼r den besten Weg zu entscheiden.

#### ðŸ¤– Roboter im Raum
Ein Roboter erhÃ¤lt Belohnung nur im Zielraum F. Mithilfe von Q-Learning lernt er, welchen Pfad er bevorzugen sollte â€“ den mit dem hÃ¶chsten erwarteten Q-Wert.

---

## Deep Q-Learning

Wenn der Zustandsraum sehr groÃŸ oder kontinuierlich ist, wird die Q-Tabelle unpraktikabel. Hier kommt **Deep Q-Learning (DQN)** zum Einsatz: Ein neuronales Netz approximiert die Q-Funktion.

Vorteile:
- Spart Speicherplatz
- ErmÃ¶glicht Einsatz bei komplexen Aufgaben, z.â€¯B.:
  - Spiele wie Breakout oder Pong
  - Bewegungssteuerung physischer Roboter

> DQN wurde 2015 durch DeepMind bekannt und zeigte, dass ein Agent auf menschlichem Niveau Atari-Spiele spielen kann (Mnih et al., 2015).

---

## NÃ¼tzliche Tools und Ressourcen

- ðŸ§ª **OpenAI Gym:** Simulationsumgebung fÃ¼r RL-Experimente  
  â†’ https://gym.openai.com/  
- ðŸ§  **MushroomRL:** Python-Bibliothek mit vielen RL-Algorithmen  
  â†’ https://mushroomrl.readthedocs.io/en/latest/  
- ðŸŽ¥ **YouTube-Tutorialreihe (deutsch):**  
  [Teil 1](https://www.youtube.com/watch?v=pc-H4vyg2L4) â€“ [Teil 2](https://www.youtube.com/watch?v=0ODB_DvMiDI) â€“ [Teil 3](https://www.youtube.com/watch?v=7cF3VzP5EDI) â€“ [Teil 4](https://www.youtube.com/watch?v=Wypc1a-1ZYA)

---

## Quellen

1. DatabaseCamp â€“ *Q-Learning einfach erklÃ¤rt*  
   [https://databasecamp.de/en/ml/q-learnings](https://databasecamp.de/en/ml/q-learnings)  
2. Vorlesungsfolien TH Augsburg â€“ *Reinforcement Learning*  
   - 380_RL-2.pdf  
   - 381_RL_SideScroller-2.pdf  
3. Mnih et al. (2015): *Human-level control through deep reinforcement learning*,  
   Nature, Vol. 518, [DOI: 10.1038/nature14236](https://doi.org/10.1038/nature14236)  
4. Wikipedia â€“ [Reinforcement Learning (englisch)](https://en.wikipedia.org/wiki/Reinforcement_learning)

